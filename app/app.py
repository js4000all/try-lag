import os
import typing as ty
import uuid

import chromadb
import google.generativeai as genai
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
import streamlit as st

from validation import validate


DEFAULT_API_KEY = os.getenv("GEMINI_API_KEY")
CHROMA_PERSIST_DIR = "/tmp/chroma_db"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50

st.set_page_config(
    page_title="RAG Demo",
    page_icon="ğŸ“š",
    layout="wide"
)

# ãƒãƒ£ãƒƒãƒˆå…¥åŠ›æ¬„ã‚’å›ºå®šã™ã‚‹ãŸã‚ã®CSS
st.markdown("""
<style>
    .stChatInput {
        position: fixed;
        bottom: 0;
        right: 0;
        width: 70%;
        z-index: 1000;
        padding: 1rem;
        box-shadow: 0 -2px 10px rgba(0,0,0,0.1);
    }
    .main .block-container {
        padding-bottom: 5rem;
    }
</style>
""", unsafe_allow_html=True)

chroma_client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'model' not in st.session_state:
    st.session_state.model = None
if 'api_key' not in st.session_state:
    st.session_state.api_key = DEFAULT_API_KEY
if 'embedding' not in st.session_state:
    st.session_state.embedding = HuggingFaceEmbeddings(model_name="cl-nagoya/ruri-v3-130m")
if 'session_id' not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
if 'messages' not in st.session_state:
    st.session_state.messages = []

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«
AVAILABLE_MODELS = [
    "gemini-2.0-flash-lite",
    "gemini-2.0-flash"
]

st.title("ğŸ“š RAG Demo")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("è¨­å®š")
    st.write(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {st.session_state.session_id}")

    api_key = st.text_input("Gemini API Key", type="password", value=st.session_state.api_key)
    st.write("https://aistudio.google.com/app/apikey")
    
    selected_model = st.selectbox(
        "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        options=list(AVAILABLE_MODELS),
        format_func=lambda x: x
    )
    
    if st.button("ãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™"):
        if api_key:
            try:
                genai.configure(api_key=api_key)
                st.session_state.model = genai.GenerativeModel(selected_model)
                st.success("ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸ")
            except Exception as e:
                st.error(f"ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        else:
            st.warning("API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    
    if st.session_state.model is not None:
        st.write(f"ãƒ¢ãƒ‡ãƒ«: {st.session_state.model.model_name}")
    else:
        st.write("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")

    max_length = st.slider("æœ€å¤§é•·", 64, 512, 256)
    temperature = st.slider("æ¸©åº¦", 0.0, 1.0, 0.7)

def _get_call_model_function() -> ty.Optional[ty.Callable[[str], str]]:
    if st.session_state.model is None:
        return None
    def _call_model(prompt: str) -> str:
        return st.session_state.model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
            max_output_tokens=max_length,
            temperature=temperature
            )
        ).text
    return _call_model

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
tab1, tab2, tab3 = st.tabs(["è³ªå•", "ãƒ‡ãƒ¼ã‚¿æŠ•å…¥", "test"])

with tab1:
    # ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.write(message["content"])

    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›ï¼ˆå›ºå®šï¼‰
    if prompt := st.chat_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", key="chat_input"):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
        with chat_container:
            with st.chat_message("user"):
                st.write(prompt)
            st.session_state.messages.append({"role": "user", "content": prompt})

        if st.session_state.model:
            try:
                call_model = _get_call_model_function()
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ¤œè¨¼
                is_valid, error_message = validate(prompt, call_model)
                if not is_valid:
                    with chat_container:
                        with st.chat_message("assistant"):
                            st.error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä¸é©åˆ‡ã§ã™: {error_message}")
                    st.session_state.messages.append({"role": "assistant", "content": f"ã‚¨ãƒ©ãƒ¼: {error_message}"})
                    st.stop()
                
                context_text = ""
                if 'vector_db' in st.session_state:
                    docs = st.session_state.vector_db.similarity_search(prompt, k=3)
                    context_text = "\n\n".join([doc.page_content for doc in docs])
                else:
                    with chat_container:
                        with st.chat_message("assistant"):
                            st.warning("æ¤œç´¢å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚’å…ˆã«è¡Œã£ã¦ãã ã•ã„ã€‚")
                    st.session_state.messages.append({"role": "assistant", "content": "æ¤œç´¢å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚’å…ˆã«è¡Œã£ã¦ãã ã•ã„ã€‚"})
                    st.stop()

                system_prompt = f"""
ä»¥ä¸‹ã®æ–‡æ›¸ã«åŸºã¥ã„ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
--- æ–‡æ›¸æƒ…å ± ---
{context_text}
--- è³ªå• ---
{prompt}
"""

                with chat_container:
                    with st.chat_message("assistant"):
                        with st.spinner("è€ƒãˆä¸­..."):
                            response = call_model(system_prompt)
                            st.write(response)
                            st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                with chat_container:
                    with st.chat_message("assistant"):
                        st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                st.session_state.messages.append({"role": "assistant", "content": f"ã‚¨ãƒ©ãƒ¼: {str(e)}"})
        else:
            with chat_container:
                with st.chat_message("assistant"):
                    st.warning("ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ãã ã•ã„")
            st.session_state.messages.append({"role": "assistant", "content": "ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ãã ã•ã„"})

with tab2:
    st.header("ãƒ‡ãƒ¼ã‚¿æŠ•å…¥")
    uploaded_file = st.file_uploader("ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["txt", "md"])
    
    if uploaded_file is not None:
        text = uploaded_file.read().decode("utf-8")
        st.text_area("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ", text, height=200, disabled=True)

        call_model = _get_call_model_function()
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP
        )
        chunks = splitter.split_text(text)
        st.write(f"ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")
        for i, chunk in enumerate(chunks):
            st.text_area(f"ãƒãƒ£ãƒ³ã‚¯ {i}", chunk, height=100, disabled=True)
            is_valid, error_message = validate(chunk, call_model)
            if not is_valid:
                st.error(f"ãƒãƒ£ãƒ³ã‚¯ã«ä¸é©åˆ‡ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {error_message}")
                st.stop()

        db = Chroma(
            client=chroma_client,
            collection_name=st.session_state.session_id,
            embedding_function=st.session_state.embedding
        )
        db.add_texts(
            texts=chunks,
            metadatas=[{"source": uploaded_file.name} for _ in range(len(chunks))],
        )
        st.success("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ–‡æ›¸ã‚’ç™»éŒ²ã—ã¾ã—ãŸï¼")
        # ä¿å­˜ã•ã‚ŒãŸDBã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿æŒ
        st.session_state.vector_db = db

with tab3:
    st.header("test")
    target = st.text_area("target", height=100)
    if st.button("validate"):
        is_valid, error_message = validate(target, _get_call_model_function())
        st.write(is_valid)
        st.write(error_message)
