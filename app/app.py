import os
import tempfile
import typing as ty

import streamlit as st
import google.generativeai as genai
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

from validation import validate


DEFAULT_API_KEY = os.getenv("GEMINI_API_KEY")

st.set_page_config(
    page_title="RAG Demo",
    page_icon="ğŸ“š",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'model' not in st.session_state:
    st.session_state.model = None

if 'api_key' not in st.session_state:
    st.session_state.api_key = DEFAULT_API_KEY

if 'embedding' not in st.session_state:
    st.session_state.embedding = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«
AVAILABLE_MODELS = [
    "gemini-2.0-flash-lite",
    "gemini-2.0-flash"
]

st.title("ğŸ“š RAG Demo")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("è¨­å®š")
    api_key = st.text_input("Gemini API Key", type="password", value=st.session_state.api_key)
    st.write("https://aistudio.google.com/app/apikey")
    
    selected_model = st.selectbox(
        "ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ",
        options=list(AVAILABLE_MODELS),
        format_func=lambda x: x
    )
    
    if st.button("ãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™"):
        if api_key:
            try:
                genai.configure(api_key=api_key)
                st.session_state.model = genai.GenerativeModel(selected_model)
                st.success("ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸ")
            except Exception as e:
                st.error(f"ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
        else:
            st.warning("API Keyã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    
    if st.session_state.model is not None:
        st.write(f"ãƒ¢ãƒ‡ãƒ«: {st.session_state.model.model_name}")
    else:
        st.write("ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")

    max_length = st.slider("æœ€å¤§é•·", 50, 500, 100)
    temperature = st.slider("æ¸©åº¦", 0.0, 1.0, 0.7)

def _get_call_model_function() -> ty.Optional[ty.Callable[[str], str]]:
    if st.session_state.model is None:
        return None
    def _call_model(prompt: str) -> str:
        return st.session_state.model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
            max_output_tokens=max_length,
            temperature=temperature
            )
        ).text
    return _call_model

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
tab1, tab2, tab3 = st.tabs(["è³ªå•", "ãƒ‡ãƒ¼ã‚¿æŠ•å…¥", "test"])

with tab1:
    st.header("è³ªå•")
    question = st.text_area("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", height=100)
    
    if st.button("å›ç­”ã‚’ç”Ÿæˆ"):
        if question and st.session_state.model:
            try:
                _call_model = _get_call_model_function()
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ¤œè¨¼
                is_valid, error_message = validate(question, _call_model)
                if not is_valid:
                    st.error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä¸é©åˆ‡ã§ã™: {error_message}")
                    st.stop()
                
                context_text = ""
                if 'vector_db' in st.session_state:
                    docs = st.session_state.vector_db.similarity_search(question, k=3)
                    context_text = "\n\n".join([doc.page_content for doc in docs])
                else:
                    st.warning("æ¤œç´¢å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ¼ã‚¿æŠ•å…¥ã‚’å…ˆã«è¡Œã£ã¦ãã ã•ã„ã€‚")

                prompt = f"""
ä»¥ä¸‹ã®æ–‡æ›¸ã«åŸºã¥ã„ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
--- æ–‡æ›¸æƒ…å ± ---
{context_text}
--- è³ªå• ---
{question}
"""

                response = _call_model(prompt)

                # å›ç­”ã®æ¤œè¨¼
                is_valid, error_message = validate(response.text, _call_model)
                if not is_valid:
                    st.error(f"å›ç­”ã«ä¸é©åˆ‡ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {error_message}")
                    st.stop()

                st.write("å›ç­”:")
                st.write(response.text)
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        else:
            if not st.session_state.model:
                st.warning("ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ã—ã¦ãã ã•ã„")
            if not question:
                st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

with tab2:
    st.header("ãƒ‡ãƒ¼ã‚¿æŠ•å…¥")
    uploaded_file = st.file_uploader("ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["txt"])
    
    if uploaded_file is not None:
        text = uploaded_file.read().decode("utf-8")
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œè¨¼
        is_valid, error_message = validate(text, _get_call_model_function())
        if not is_valid:
            st.error(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã«ä¸é©åˆ‡ãªå†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™: {error_message}")
            st.stop()
            
        st.text_area("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ", text, height=200)

        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        chunks = splitter.split_text(text)
        st.write(f"ãƒãƒ£ãƒ³ã‚¯æ•°: {len(chunks)}")

        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆChromaDBã®æ°¸ç¶šåŒ–å ´æ‰€ï¼‰
        with tempfile.TemporaryDirectory() as persist_dir:
            db = Chroma.from_texts(
                texts=chunks,
                embedding=st.session_state.embedding,
                persist_directory=persist_dir
            )
            st.success("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ–‡æ›¸ã‚’ç™»éŒ²ã—ã¾ã—ãŸï¼")
            # ä¿å­˜ã•ã‚ŒãŸDBã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿æŒ
            st.session_state.vector_db = db

with tab3:
    st.header("test")
    target = st.text_area("target", height=100)
    if st.button("validate"):
        is_valid, error_message = validate(target, _get_call_model_function())
        st.write(is_valid)
        st.write(error_message)
